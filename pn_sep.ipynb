{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN(\n",
      "  (fc): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self,num_layers,num_nodes):\n",
    "        super(FCN,self).__init__()\n",
    "        if num_layers!=len(num_nodes)-1:\n",
    "            sys.exit(\"Miss Match on number of layers\")\n",
    "        self.num_lay=num_layers\n",
    "        self.num_nodes=num_nodes\n",
    "        self.fc=nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.fc.append(nn.Linear(num_nodes[i],num_nodes[i+1]))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for i in range(self.num_lay-1):\n",
    "            x=F.relu(self.fc[i](x))\n",
    "        x=self.fc[self.num_lay-1](x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def print_param(self):\n",
    "        net_W=[]\n",
    "        net_b=[]\n",
    "        for i in range(self.num_lay):\n",
    "            net_W.append(self.fc[i].weight.data)\n",
    "            net_b.append(self.fc[i].bias.data)\n",
    "            #print(self.fc[i].weight.data)\n",
    "            #print(self.fc[i].bias.data)\n",
    "            \n",
    "        return [net_W,net_b]\n",
    "        \n",
    "    \n",
    "\n",
    "fcn_i1=FCN(3,[784,200,200,10])\n",
    "print(fcn_i1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/anaconda3/envs/ece587_ptc/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.300741\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 2.167991\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 1.910270\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 1.320376\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.750749\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.684368\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.453436\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.409497\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.328820\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.419852\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.327281\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.366134\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.362674\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.341502\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.314155\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.230919\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.369818\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.266780\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.444458\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.296334\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.254342\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.341871\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.224174\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.272532\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.254027\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.308977\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.328839\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.215021\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.219448\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.211493\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.238799\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.247406\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.270739\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.133175\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.212704\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.140744\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.184075\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.156990\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.220669\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.238973\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.176790\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.212895\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.159986\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.139663\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.206404\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.199161\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.182677\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.186477\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.185383\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.253517\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.224305\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.190951\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.185835\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.160867\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.226806\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.242445\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.168270\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.239207\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.142102\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.168949\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.149390\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.220150\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.261800\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.254195\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.130490\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.158691\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.158807\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.169933\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.215350\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.116997\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.149734\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.121322\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.191795\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.182670\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.122424\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.109542\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.144320\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.121847\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.087280\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.210240\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.110075\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.121090\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.138950\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.231447\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.124731\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.128128\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.118751\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.116317\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.171152\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.066473\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.110948\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.127172\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.137745\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.075645\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.097277\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.149302\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.124557\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.104132\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.078493\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.112260\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.094082\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.082541\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.148694\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.049642\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.079143\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.114098\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.152830\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.135739\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.058481\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.068437\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.140205\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.119520\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.079250\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.068995\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.063424\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.066593\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.135332\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.121808\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.115204\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.141152\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.120367\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.055637\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.096621\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.046015\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.096157\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.063933\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.069429\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.079881\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.066580\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.070623\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.059863\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.133493\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.099284\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.072382\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.076378\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.135659\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.114682\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.138841\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.058967\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.127459\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.110920\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.083464\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.073364\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.110080\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.054604\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.038763\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.060872\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.108194\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.077912\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.114018\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.026122\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.071510\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.052794\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.100265\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.030771\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.068657\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.038643\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.103167\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.069503\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.043731\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.050216\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.077142\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.080315\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.078290\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.064074\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.053619\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.123845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.096239\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.097009\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.074973\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.087927\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.066745\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.021033\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.044207\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.035209\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.077830\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.076833\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.047803\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.090826\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.085127\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.023138\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 0.058082\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.030652\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.083233\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.039750\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.053032\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.076383\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 0.096533\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.047169\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.048739\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.107452\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 0.097380\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.037601\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 0.045213\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.044732\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.030334\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.121689\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 0.045037\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.087202\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 0.076537\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.037379\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.039708\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.022798\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 0.053868\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.082615\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.053127\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.100983\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.043690\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.099580\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 0.057477\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.040837\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 0.036993\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.061779\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.077675\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.064536\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.036840\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.065496\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 0.049473\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.037411\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.044833\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.103882\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 0.039539\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.054709\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 0.052598\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.047458\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.034115\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.100739\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 0.043498\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.073006\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 0.033332\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.083411\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.025442\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.073518\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 0.033314\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.028835\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.021705\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.039982\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.062516\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.049942\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 0.044224\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.045100\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.043207\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.024663\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.038209\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.028928\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.013540\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.023339\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 0.060535\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.049667\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.041223\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.053617\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 0.014870\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.045248\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 0.034011\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.058631\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.019206\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.028167\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 0.051583\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.033241\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 0.023721\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.067023\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.014920\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.019772\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 0.049384\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.034515\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.012564\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.049839\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.028533\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.079920\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 0.042072\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.053117\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 0.024166\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.019481\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.017709\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.100687\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.022978\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.031344\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 0.024712\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.042492\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.026236\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.035864\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.054554\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.111353\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 0.048553\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.017761\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.078673\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.019328\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 0.038609\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.027656\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 0.043203\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.024249\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.049592\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.046742\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 0.036194\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.064843\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.033957\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.019997\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.057537\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.034097\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 0.019387\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "epochs=10\n",
    "batch_size=200\n",
    "log_interval=10\n",
    "optimizer = optim.SGD(fcn_i1.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "            data = data.view(-1, 28*28)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = fcn_i1(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/anaconda3/envs/ece587_ptc/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 9789/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = fcn_i1(data)\n",
    "    #print(net_out)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensor_comprehensions as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n_W,n_b]=fcn_i1.print_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lang = \"\"\"\n",
    "#def fcrelu(float(B,M) I, float(N,M) W1, float(N) B1) -> (O1) {\n",
    "#    O1(b, n) +=! I(b, m) * W1(n, m)\n",
    "#    O1(b, n) = O1(b, n) + B1(n)\n",
    "#    O1(b, n) = fmax(O1(b, n), 0)\n",
    "#}\n",
    "#\"\"\"\n",
    "\n",
    "#lang1 = \"\"\"\n",
    "#def fc(float(B,M) I, float(N,M) W1,float(N) B1) -> (O1){\n",
    "#     O1(b, n)+=! I(b,m)* W1(n, m)\n",
    "#     O1(b, n) = O1(b, n) + B1(n)\n",
    "#}\n",
    "#\"\"\"\n",
    "\n",
    "#lang2=\"\"\"\n",
    "#def softmax(float(N, D) I) -> (O, maxVal, expDistance, expSum) {\n",
    "#    maxVal(n) max=! I(n, d)\n",
    "#    expDistance(n, d) = exp(I(n, d) - maxVal(n))\n",
    "#    expSum(n) +=! expDistance(n, d)\n",
    "#    O(n, d) = expDistance(n, d) / expSum(n)\n",
    "#}\n",
    "#\"\"\"\n",
    "\n",
    "#fcrelu = tc.define(lang, name=\"fcrelu\")\n",
    "#fc=tc.define(lang1,name=\"fc\")\n",
    "#softmax=tc.define(lang2,name=\"softmax\")\n",
    "\n",
    "lang = \"\"\"\n",
    "def fcrelunet(float(B,M) I, float(N,M) W1, float(N) B1,float(P,N) W2, float(P) B2,float(Q,P) W3, float(Q) B3) -> (O1,O2,O3,O4,O5,maxVal, expDistance, expSum) {\n",
    "    O1(b, n) +=! I(b, m) * W1(n, m)\n",
    "    O1(b, n) = O1(b, n) + B1(n)\n",
    "    O1(b, n) = fmax(O1(b, n), 0)\n",
    "    O2(b, p) +=! O1(b, n) * W2(p, n)\n",
    "    O2(b, p) = O2(b, p) + B2(p)\n",
    "    O2(b, p) = fmax(O2(b, p), 0)\n",
    "    O3(b, q) +=! O2(b, p) * W3(q, p)\n",
    "    O3(b, q) = O3(b, q) + B3(q)\n",
    "    maxVal(b) max=! O3(b , q)\n",
    "    expDistance(b ,q) = exp(O3(b , q) - maxVal(b))\n",
    "    expSum(b) +=! expDistance(b , q)\n",
    "    O4(b , q) = expDistance(b , q) / expSum(b)\n",
    "    O5(b, q) = log(O4(b , q))\n",
    "}\n",
    "\"\"\"\n",
    "#O5(b , q) = log(O4(b , q))\n",
    "fcrelunet = tc.define(lang, name=\"fcrelunet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Autotuning cache will be saved to: fcrelu_784_200_200_10.tc.cuda/options\n",
      "[INFO]: Tuned kernel options found, using those options\n"
     ]
    }
   ],
   "source": [
    "#B_1,B_2, M, N = 28*28,1, 200, 200\n",
    "# I= torch.ones(B_2, B_1).cuda() \n",
    "#W1, B1=torch.ones(M, B_1).cuda(), torch.ones(N).cuda()\n",
    "#W1=n_W[0].cuda()\n",
    "#B1=n_b[0].cuda()\n",
    "#out1=torch.ones(B_2,N)\n",
    "#W2=n_W[1].cuda()\n",
    "#B2=n_b[1].cuda()\n",
    "#W3=n_W[2].cuda()\n",
    "#B3=n_b[2].cuda()\n",
    "\n",
    "#fcrelu.autotune(I, W1, B1, cache=\"fcrelu_784_200_200.tc\")\n",
    "#out = fcrelu(I, W1, B1)\n",
    "\n",
    "ID,B,M,N,P,Q=1,28*28,200,200,200,10\n",
    "I= torch.ones(ID, B).cuda()\n",
    "#W1=torch.ones(M, B).cuda()\n",
    "#B1=torch.ones(N).cuda()\n",
    "#W2=torch.ones(P, N).cuda()\n",
    "#B2=torch.ones(P).cuda() \n",
    "#W3=torch.ones(Q, P).cuda()\n",
    "#B3=torch.ones(Q).cuda()\n",
    "W1=n_W[0].cuda()\n",
    "B1=n_b[0].cuda()\n",
    "W2=n_W[1].cuda()\n",
    "B2=n_b[1].cuda()\n",
    "W3=n_W[2].cuda()\n",
    "B3=n_b[2].cuda()\n",
    "#print(I.size())\n",
    "#print(W1.size())\n",
    "#print(B1.size())\n",
    "#print(W2.size())\n",
    "#print(B2.size())\n",
    "#print(W3.size())\n",
    "#print(B3.size())\n",
    "#print(\"Actual Sizes\")\n",
    "#print(n_W[0].size())\n",
    "#print(n_b[0].size())\n",
    "#print(n_W[1].size())\n",
    "#print(n_b[1].size())\n",
    "#print(n_W[2].size())\n",
    "#print(n_b[2].size())\n",
    "fcrelunet.autotune(I, W1, B1, W2 , B2, W3, B3, cache=\"fcrelu_784_200_200_10.tc\")\n",
    "out=fcrelunet(I,W1,B1,W2,B2,W3,B3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-4.0681 -3.0137 -3.4088 -1.5117 -7.9687 -0.4889 -5.0192 -2.9962 -6.7298 -4.7391\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Time on 10 iterations for cpu:\n",
      "10.2730656147\n",
      "Max Time on 10 iterations for cpu:\n",
      "97.6877441406\n",
      "Min Time on 10 iterations for cpu:\n",
      "0.514496028423\n",
      "Variable containing:\n",
      "-4.0681 -3.0137 -3.4088 -1.5117 -7.9687 -0.4889 -5.0192 -2.9962 -6.7298 -4.7391\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/anaconda3/envs/ece587_ptc/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "d=Variable(torch.ones(ID,B),requires_grad=False)\n",
    "time=[]\n",
    "for i in range(10):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    n_out=fcn_i1(d)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    time.append(start.elapsed_time(end))\n",
    "\n",
    "time=np.array(time)\n",
    "print('Mean Time on 10 iterations for cpu:')\n",
    "print(np.mean(time))\n",
    "print('Max Time on 10 iterations for cpu:')\n",
    "print(np.max(time))\n",
    "print('Min Time on 10 iterations for cpu:')\n",
    "print(np.min(time))\n",
    "print(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
